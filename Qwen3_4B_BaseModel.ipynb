{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "mount_file_id": "1sdYIGZ7k9hPf-Dar71kcfDgCkXZoetSv",
      "authorship_tag": "ABX9TyPRoloXf89XLEI0R2aO0K4a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maximi652/efficient-slm-architectures/blob/main/Qwen3_4B_BaseModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install --upgrade transformers accelerate"
      ],
      "metadata": {
        "id": "kyIbd1lBXOVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRotYFP2WaMX"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "# Pfade und Modellname\n",
        "MODEL_PATH    = \"/content/drive/MyDrive/Colab Notebooks/Qwen3-4B\"\n",
        "INPUT_JSON    = \"/content/drive/MyDrive/Colab Notebooks/12B_combined_golden.json\"\n",
        "OUTPUT_JSON   = \"/content/drive/MyDrive/Colab Notebooks/base-qwen3-4b_testresults.json\"\n",
        "\n",
        "# Tokenizer & Modell laden\n",
        "model_name = MODEL_PATH\n",
        "\n",
        "# Lade Tokenizer und Modell mit Chat-Template Support\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# Use left padding to avoid decoder-only right-padding issues\n",
        "tokenizer.padding_side = 'left'\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Generation-Konfiguration (keine Gedanken)\n",
        "gen_conf = GenerationConfig(\n",
        "    max_new_tokens=200,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "# Hilfsfunktionen für Prompt-Bau und Cleaning\n",
        "def build_messages(qtext, snippets, qtype, mode=\"exact\"):\n",
        "    \"\"\"\n",
        "    Erzeugt die Chat-Message-Liste mit system directive /no_think\n",
        "    und user content entsprechend modus.\n",
        "    \"\"\"\n",
        "    # System directive zum Deaktivieren des Denkens\n",
        "    system_msg = {\"role\":\"system\",\"content\":\"/no_think\"}\n",
        "    # Context-Snippets\n",
        "    ctx = \"\\n\".join(s[\"text\"] for s in snippets[:2])\n",
        "    if mode == \"exact\":\n",
        "        if qtype == \"yesno\":\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nAnswer only 'yes' or 'no', in English, no extras.\"\n",
        "        elif qtype == \"factoid\":\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide up to 5 keywords, comma-separated, in English, no commentary.\"\n",
        "        elif qtype == \"list\":\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a comma-separated list of relevant items, in English, no filler words.\"\n",
        "        else:\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a brief answer in English.\"\n",
        "    else:  # ideal\n",
        "        if qtype == \"yesno\":\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide one-sentence ideal answer in English starting with 'Yes,' or 'No,'.\"\n",
        "        else:\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide an ideal answer in English (one paragraph, max 200 words, full sentences).\"\n",
        "    # User message\n",
        "    user_msg = {\"role\":\"user\",\"content\":content}\n",
        "    return [system_msg, user_msg]\n",
        "\n",
        "\n",
        "def clean_exact(text, qtype):\n",
        "    txt = text.strip()\n",
        "    # Remove any thinking tags or filler\n",
        "    txt = re.sub(r'<\\/think>','', txt)\n",
        "    txt = re.sub(r'\\s*(Okay\\.?|etc\\.?|usw\\.?|\\.\\.\\.)$', '', txt, flags=re.IGNORECASE)\n",
        "    if qtype == \"yesno\":\n",
        "        return \"yes\" if txt.lower().startswith(\"yes\") else \"no\"\n",
        "    if qtype in (\"factoid\",\"list\"):\n",
        "        items = [i.strip() for i in txt.split(\",\") if i.strip()]\n",
        "        return items\n",
        "    return None\n",
        "\n",
        "\n",
        "def clean_ideal(text, qtype):\n",
        "    txt = text.strip()\n",
        "    txt = re.sub(r'<\\/think>','', txt)\n",
        "    txt = re.sub(r'\\s*(Okay\\.?|etc\\.?|usw\\.?|\\.\\.\\.)$', '', txt, flags=re.IGNORECASE)\n",
        "    # truncate to full sentences\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', txt)\n",
        "    if qtype == \"yesno\":\n",
        "        return sentences[0].strip()\n",
        "    # for summary/list/factoid, join until ~200 words\n",
        "    total = 0\n",
        "    out = []\n",
        "    for sent in sentences:\n",
        "        length = len(sent.split())\n",
        "        if total + length <= 200:\n",
        "            out.append(sent)\n",
        "            total += length\n",
        "        else:\n",
        "            break\n",
        "    return \" \".join(out).strip()\n",
        "\n",
        "# Datensatz laden\n",
        "with open(INPUT_JSON, 'r', encoding='utf-8') as f:\n",
        "    questions = json.load(f)['questions']\n",
        "\n",
        "# Batch-Inferenz\n",
        "batch_size = 8\n",
        "submission = []\n",
        "\n",
        "for i in tqdm(range(0, len(questions), batch_size), desc='Batches'):\n",
        "    batch = questions[i:i+batch_size]\n",
        "    # exact\n",
        "    msgs_ex = [build_messages(q['body'], q.get('snippets',[]), q['type'], mode='exact') for q in batch]\n",
        "    texts_ex = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True, enable_thinking=False) for m in msgs_ex]\n",
        "    inputs_ex = tokenizer(texts_ex, return_tensors='pt', padding=True, truncation=True).to(model.device)\n",
        "    # Alternatively:\n",
        "    # raw_ex = tokenizer(texts_ex, return_tensors='pt', padding=True, truncation=True)\n",
        "    # inputs_ex = {k: v.to(model.device) for k, v in raw_ex.items()}\n",
        "    with torch.no_grad():\n",
        "        out_ex = model.generate(**inputs_ex, generation_config=gen_conf)\n",
        "    # decode exact\n",
        "    dec_ex = []\n",
        "    for idx, q in enumerate(batch):\n",
        "        start = inputs_ex['input_ids'].shape[1]\n",
        "        ids = out_ex[idx][start:].tolist()\n",
        "        text = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "        dec_ex.append(clean_exact(text, q['type']))\n",
        "    # ideal\n",
        "    msgs_id = [build_messages(q['body'], q.get('snippets',[]), q['type'], mode='ideal') for q in batch]\n",
        "    texts_id = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True, enable_thinking=False) for m in msgs_id]\n",
        "    inputs_id = tokenizer(texts_id, return_tensors='pt', padding=True, truncation=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out_id = model.generate(**inputs_id, generation_config=gen_conf)\n",
        "    dec_id = []\n",
        "    for idx, q in enumerate(batch):\n",
        "        start = inputs_id['input_ids'].shape[1]\n",
        "        ids = out_id[idx][start:].tolist()\n",
        "        text = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "        dec_id.append(clean_ideal(text, q['type']))\n",
        "    # merge\n",
        "    for idx, q in enumerate(batch):\n",
        "        submission.append({\n",
        "            'id': q['id'],\n",
        "            'type': q['type'],\n",
        "            'exact_answer': q.get('exact_answer'),\n",
        "            'ideal_answer': q.get('ideal_answer'),\n",
        "            'exact_prediction': dec_ex[idx],\n",
        "            'ideal_prediction': dec_id[idx]\n",
        "        })\n",
        "\n",
        "# Submission speichern\n",
        "with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
        "    json.dump(submission, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print('✅ Submission file created at:', OUTPUT_JSON)"
      ]
    }
  ]
}