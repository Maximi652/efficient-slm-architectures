{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "https://github.com/Maximi652/efficient-slm-architectures/blob/main/Qwen3-4B_LoRA.ipynb",
      "authorship_tag": "ABX9TyPUkjll7WZBDfxpwh3VjUvQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maximi652/efficient-slm-architectures/blob/main/Qwen3-4B_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning LoRA"
      ],
      "metadata": {
        "id": "yHiXLeYqaYXn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM8G8w5WSctf"
      },
      "outputs": [],
      "source": [
        "# libs\n",
        "!pip install -q transformers datasets peft accelerate\n",
        "\n",
        "import json\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
        "import torch\n",
        "\n",
        "# Model\n",
        "model_name = \"/content/drive/MyDrive/Colab Notebooks/Qwen3-4B\"\n",
        "\n",
        "# train\n",
        "json_path = \"/content/drive/MyDrive/Colab Notebooks/12B_trainingdata.json\"\n",
        "\n",
        "# tokenzizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# Use left padding to avoid decoder-only right-padding issues\n",
        "tokenizer.padding_side = 'left'\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Modell\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# chat Template, /no_think\n",
        "def format_entry_chat(entry):\n",
        "\n",
        "    qtext = entry[\"body\"].strip()\n",
        "    qtype = entry.get(\"type\", \"factoid\").lower()\n",
        "    ctx = \"\\n\".join(s[\"text\"].strip() for s in entry.get(\"snippets\", []))\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # ausformulierte, ganze antwort\n",
        "    ideal_ans = entry.get(\"ideal_answer\", \"\")[0].strip()\n",
        "    if ideal_ans:\n",
        "        if qtype == \"yesno\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide one-sentence ideal answer in English starting with 'Yes,' or 'No,'.\"\n",
        "        else:\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide an ideal answer in English (one paragraph, max 200 words, full sentences).\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "            {\"role\": \"assistant\", \"content\": ideal_ans}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "            enable_thinking=False\n",
        "        )\n",
        "        results.append({\"text\": text})\n",
        "\n",
        "    # kurze, knappe Antwort\n",
        "    exact = entry.get(\"exact_answer\", [])\n",
        "    flat_exact = [item[0] if isinstance(item, list) and item else item for item in exact]\n",
        "    exact_ans = \", \".join(flat_exact).strip()\n",
        "\n",
        "    if exact_ans:\n",
        "        if qtype == \"yesno\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nAnswer only 'yes' or 'no', in English, no extras.\"\n",
        "        elif qtype == \"factoid\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide up to 5 keywords, comma-separated, in English, no commentary.\"\n",
        "        elif qtype == \"list\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a comma-separated list of relevant items, in English, no filler words.\"\n",
        "        else:\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a brief answer in English.\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "            {\"role\": \"assistant\", \"content\": exact_ans}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "            enable_thinking=False\n",
        "        )\n",
        "        results.append({\"text\": text})\n",
        "\n",
        "    return results\n",
        "\n",
        "# Train laden\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)[\"questions\"]\n",
        "\n",
        "formatted = []\n",
        "for entry in raw_data:\n",
        "    formatted.extend(format_entry_chat(entry))\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(formatted)\n",
        "})\n",
        "\n",
        "# tokenisieren\n",
        "def tokenize(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=1024 # ggf 2048, falls GPU-RAM zulässt\n",
        "    )\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Start LoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.gradient_checkpointing_enable() # RAM sparen\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen3-4b-lora-nothink\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=3,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    eval_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_steps=25,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# save\n",
        "model.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/qwen3-4b-lora-nothink\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/qwen3-4b-lora-nothink\")\n",
        "\n",
        "print(\"Training abgeschlossen – Modell unter ./qwen3-4b-lora-nothink gespeichert.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferenz"
      ],
      "metadata": {
        "id": "ZjkKlogAaVRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install und Imports\n",
        "!pip install -q transformers datasets peft accelerate\n",
        "\n",
        "import json\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from torch import inference_mode\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# Pfade und Gerät\n",
        "model_name = \"/content/drive/MyDrive/Colab Notebooks/Qwen3-4B\"\n",
        "lora_adapter_path = \"/content/drive/MyDrive/Colab Notebooks/qwen3-4b-lora-nothink\"\n",
        "test_json_path = \"/content/drive/MyDrive/Colab Notebooks/12b_golden_testdata.json\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.padding_side = \"left\"\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Basis-Modell und LoRA-Adapter\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, lora_adapter_path, torch_dtype=\"auto\")\n",
        "model.gradient_checkpointing_disable()\n",
        "model.eval()\n",
        "model = torch.compile(model)\n",
        "model.to(device)\n",
        "\n",
        "gen_conf = GenerationConfig(\n",
        "    max_new_tokens=200,   # Limitiere die generierten Tokens\n",
        "    do_sample=False,      # Greedy- statt Sample-Decoding\n",
        "    use_cache=True,       # Aktiviert KV-Caching\n",
        "    early_stopping=True   # Stoppt, wenn EOS erzeugt wird\n",
        ")\n",
        "\n",
        "# Testdaten parsen und formatieren\n",
        "def format_entry_chat(entry):\n",
        "    qtext = entry[\"body\"].strip()\n",
        "    qtype = entry.get(\"type\", \"factoid\").lower()\n",
        "    ctx = \"\\n\".join(s[\"text\"].strip() for s in entry.get(\"snippets\", []))\n",
        "    results = []\n",
        "\n",
        "    # Ausformulierte Antwort\n",
        "    ideal_ans = entry.get(\"ideal_answer\", [\"\"])[0].strip()\n",
        "    if ideal_ans:\n",
        "        if qtype == \"yesno\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide one-sentence ideal answer in English starting with 'Yes,' or 'No,'.\"\n",
        "        else:\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide an ideal answer in English (one paragraph, max 200 words, full sentences).\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "            enable_thinking=False\n",
        "        )\n",
        "        results.append({\"text\": text})\n",
        "\n",
        "    # Kurzantwort\n",
        "    exact = entry.get(\"exact_answer\", [])\n",
        "    flat_exact = [item[0] if isinstance(item, list) and item else item for item in exact]\n",
        "    exact_ans = \", \".join(flat_exact).strip()\n",
        "    if exact_ans:\n",
        "        if qtype == \"yesno\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nAnswer only 'yes' or 'no', in English, no extras.\"\n",
        "        elif qtype == \"factoid\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide up to 5 keywords, comma-separated, in English, no commentary.\"\n",
        "        elif qtype == \"list\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a comma-separated list of relevant items, in English, no filler words.\"\n",
        "        else:\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a brief answer in English.\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "            enable_thinking=False\n",
        "        )\n",
        "        results.append({\"text\": text})\n",
        "\n",
        "    return results\n",
        "\n",
        "with open(test_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_test = json.load(f)[\"questions\"]\n",
        "\n",
        "# Flatten\n",
        "formatted_test = []\n",
        "for entry in raw_test:\n",
        "    formatted_test.extend(format_entry_chat(entry))\n",
        "\n",
        "# Inference\n",
        "def generate_answer(prompt_text, max_new_tokens=256, **gen_kwargs):\n",
        "    with inference_mode():\n",
        "        inputs = tokenizer(\n",
        "            prompt_text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=1024\n",
        "        ).to(device)\n",
        "        # Hier kannst du Sampling-Parameter anpassen\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            generation_config=gen_conf,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            **gen_kwargs\n",
        "        )\n",
        "        # Den generierten Text ab der Eingabelänge decodieren\n",
        "        gen = output_ids[0, inputs[\"input_ids\"].shape[-1]:]\n",
        "        return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "\n",
        "# Loop über den Testdatensatz\n",
        "results = []\n",
        "for idx, ex in enumerate(formatted_test):\n",
        "    prompt = ex[\"text\"]\n",
        "    pred = generate_answer(prompt)\n",
        "    results.append({\n",
        "        \"index\": idx,\n",
        "        \"prompt\": prompt,\n",
        "        \"prediction\": pred\n",
        "    })\n",
        "    if idx % 20 == 0:\n",
        "        print(f\"Processed {idx}/{len(formatted_test)}\")\n",
        "\n",
        "# 8. Ergebnisse speichern oder auswerten\n",
        "import json\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/test_predictions_LoRA.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Inference abgeschlossen – Ergebnisse in test_predictions_LoRA.json gespeichert.\")\n"
      ],
      "metadata": {
        "id": "RZ8LA8sKZ17A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gL36JbBUsE0n"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}