{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "https://github.com/Maximi652/efficient-slm-architectures/blob/main/Qwen3-4B_LoRA.ipynb",
      "authorship_tag": "ABX9TyPj2BZ1JvliKKjOAcbl5qAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maximi652/efficient-slm-architectures/blob/main/Qwen3-4B_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning LoRA"
      ],
      "metadata": {
        "id": "yHiXLeYqaYXn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM8G8w5WSctf"
      },
      "outputs": [],
      "source": [
        "# libs\n",
        "!pip install -q transformers datasets peft accelerate\n",
        "\n",
        "import json\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
        "import torch\n",
        "\n",
        "# Model\n",
        "model_name = \"/content/drive/MyDrive/Colab Notebooks/Qwen3-4B\"\n",
        "\n",
        "# train\n",
        "json_path = \"/content/drive/MyDrive/Colab Notebooks/12B_trainingdata.json\"\n",
        "\n",
        "# tokenzizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# Use left padding to avoid decoder-only right-padding issues\n",
        "tokenizer.padding_side = 'left'\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Modell\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# chat Template, /no_think\n",
        "def format_entry_chat(entry):\n",
        "\n",
        "    qtext = entry[\"body\"].strip()\n",
        "    qtype = entry.get(\"type\", \"factoid\").lower()\n",
        "    ctx = \"\\n\".join(s[\"text\"].strip() for s in entry.get(\"snippets\", []))\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # ausformulierte, ganze antwort\n",
        "    ideal_ans = entry.get(\"ideal_answer\", \"\")[0].strip()\n",
        "    if ideal_ans:\n",
        "        if qtype == \"yesno\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide one-sentence ideal answer in English starting with 'Yes,' or 'No,'.\"\n",
        "        else:\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide an ideal answer in English (one paragraph, max 200 words, full sentences).\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "            {\"role\": \"assistant\", \"content\": ideal_ans}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "            enable_thinking=False\n",
        "        )\n",
        "        results.append({\"text\": text})\n",
        "\n",
        "    # kurze, knappe Antwort\n",
        "    exact = entry.get(\"exact_answer\", [])\n",
        "    flat_exact = [item[0] if isinstance(item, list) and item else item for item in exact]\n",
        "    exact_ans = \", \".join(flat_exact).strip()\n",
        "\n",
        "    if exact_ans:\n",
        "        if qtype == \"yesno\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nAnswer only 'yes' or 'no', in English, no extras.\"\n",
        "        elif qtype == \"factoid\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide up to 5 keywords, comma-separated, in English, no commentary.\"\n",
        "        elif qtype == \"list\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a comma-separated list of relevant items, in English, no filler words.\"\n",
        "        else:\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a brief answer in English.\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "            {\"role\": \"assistant\", \"content\": exact_ans}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "            enable_thinking=False\n",
        "        )\n",
        "        results.append({\"text\": text})\n",
        "\n",
        "    return results\n",
        "\n",
        "# Train laden\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)[\"questions\"]\n",
        "\n",
        "formatted = []\n",
        "for entry in raw_data:\n",
        "    formatted.extend(format_entry_chat(entry))\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(formatted)\n",
        "})\n",
        "\n",
        "# tokenisieren\n",
        "def tokenize(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=1024 # ggf 2048, falls GPU-RAM zulässt\n",
        "    )\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Start LoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.gradient_checkpointing_enable() # RAM sparen\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen3-4b-lora-nothink\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=3,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    eval_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_steps=25,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# save\n",
        "model.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/qwen3-4b-lora-nothink\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/qwen3-4b-lora-nothink\")\n",
        "\n",
        "print(\"Training abgeschlossen – Modell unter ./qwen3-4b-lora-nothink gespeichert.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferenz"
      ],
      "metadata": {
        "id": "ZjkKlogAaVRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "# model + data\n",
        "MODEL_PATH = \"./qwen3-4b-lora-nothink\"\n",
        "TEST_PATH  = \"/content/drive/MyDrive/Colab Notebooks/12B_golden_testdata.json\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Colab Notebooks/qwen3-4b_lora_predictions.json\"\n",
        "\n",
        "# tokenizer + model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# gen config\n",
        "gen_conf = GenerationConfig(\n",
        "    max_new_tokens=200,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "# Prompt-Builder (wie beim Training)\n",
        "def build_messages(qtext, snippets, qtype, mode=\"ideal\"):\n",
        "    ctx = \"\\n\".join(s[\"text\"].strip() for s in snippets[:2])\n",
        "    if mode == \"exact\":\n",
        "        if qtype == \"yesno\":\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nAnswer only 'yes' or 'no', in English, no extras.\"\n",
        "        elif qtype == \"factoid\":\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide up to 5 keywords, comma-separated, in English, no commentary.\"\n",
        "        elif qtype == \"list\":\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a comma-separated list of relevant items, in English, no filler words.\"\n",
        "        else:\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a brief answer in English.\"\n",
        "    else:\n",
        "        if qtype == \"yesno\":\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide one-sentence ideal answer in English starting with 'Yes,' or 'No,'.\"\n",
        "        else:\n",
        "            content = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide an ideal answer in English (one paragraph, max 200 words, full sentences).\"\n",
        "\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "        {\"role\": \"user\", \"content\": content}\n",
        "    ]\n",
        "\n",
        "# Antworten bereinigen\n",
        "def clean_output(text, qtype, mode):\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"</think>\", \"\", text)\n",
        "    text = re.sub(r\"\\s*(Okay\\.?|etc\\.?|usw\\.?|\\.\\.\\.)$\", \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    if mode == \"exact\":\n",
        "        if qtype == \"yesno\":\n",
        "            return \"yes\" if text.lower().startswith(\"yes\") else \"no\"\n",
        "        if qtype in (\"factoid\", \"list\"):\n",
        "            return [i.strip() for i in text.split(\",\") if i.strip()]\n",
        "    else:\n",
        "        sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
        "        if qtype == \"yesno\":\n",
        "            return sentences[0].strip()\n",
        "        out = []\n",
        "        total = 0\n",
        "        for sent in sentences:\n",
        "            length = len(sent.split())\n",
        "            if total + length <= 200:\n",
        "                out.append(sent)\n",
        "                total += length\n",
        "            else:\n",
        "                break\n",
        "        return \" \".join(out).strip()\n",
        "    return text.strip()\n",
        "\n",
        "# Testdaten laden\n",
        "with open(TEST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)[\"questions\"]\n",
        "\n",
        "submission = []\n",
        "batch_size = 4\n",
        "\n",
        "# Inferenz für beide Modi\n",
        "for mode in [\"exact\", \"ideal\"]:\n",
        "    for i in tqdm(range(0, len(test_data), batch_size), desc=f\"Inferenz ({mode})\"):\n",
        "        batch = test_data[i:i+batch_size]\n",
        "        messages = [build_messages(q[\"body\"], q.get(\"snippets\", []), q[\"type\"], mode=mode) for q in batch]\n",
        "        prompts = [\n",
        "            tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n",
        "            for m in messages\n",
        "        ]\n",
        "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, generation_config=gen_conf)\n",
        "\n",
        "        for idx, q in enumerate(batch):\n",
        "            start = inputs[\"input_ids\"].shape[1]\n",
        "            ids = outputs[idx][start:].tolist()\n",
        "            decoded = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "            cleaned = clean_output(decoded, q[\"type\"], mode)\n",
        "\n",
        "            # Füge Ergebnis hinzu oder aktualisiere bestehendes\n",
        "            qid = q[\"id\"]\n",
        "            existing = next((x for x in submission if x[\"id\"] == qid), None)\n",
        "            if existing:\n",
        "                existing[f\"{mode}_prediction\"] = cleaned\n",
        "            else:\n",
        "                submission.append({\n",
        "                    \"id\": qid,\n",
        "                    \"type\": q[\"type\"],\n",
        "                    \"exact_answer\": q.get(\"exact_answer\"),\n",
        "                    \"ideal_answer\": q.get(\"ideal_answer\"),\n",
        "                    f\"{mode}_prediction\": cleaned\n",
        "                })\n",
        "\n",
        "# Ergebnisse speichern\n",
        "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(submission, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Inferenz abgeschlossen. Ergebnisse gespeichert unter: {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "id": "RZ8LA8sKZ17A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}